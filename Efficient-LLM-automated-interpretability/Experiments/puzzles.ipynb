{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import utils\n",
    "from neuron_explainer.explanations.puzzles import PUZZLES_BY_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLAINER_MODEL_NAME = \"Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': {'code': None, 'message': 'Rate limit exceeded', 'param': None, 'type': 'rate_limit_exceeded'}}\n",
      "Retrying after API error: Rate limit exceeded (https://api.sambanova.ai/v1/chat/completions)\n",
      "{'error': {'code': None, 'message': 'Rate limit exceeded', 'param': None, 'type': 'rate_limit_exceeded'}}\n",
      "Retrying after API error: Rate limit exceeded (https://api.sambanova.ai/v1/chat/completions)\n",
      "{'error': {'code': None, 'message': 'Rate limit exceeded', 'param': None, 'type': 'rate_limit_exceeded'}}\n",
      "Retrying after API error: Rate limit exceeded (https://api.sambanova.ai/v1/chat/completions)\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/workspace/Efficient-LLM-automated-interpretability/neuron-explainer/neuron_explainer/api_client.py:70\u001b[0m, in \u001b[0;36mexponential_backoff.<locals>.decorate.<locals>.f_retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/workspace/Efficient-LLM-automated-interpretability/neuron-explainer/neuron_explainer/api_client.py:140\u001b[0m, in \u001b[0;36mApiClient.make_request\u001b[0;34m(self, timeout_seconds, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mjson())\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/workspace/Efficient-LLM-automated-interpretability/neuron-explainer/neuron_explainer/api_client.py:137\u001b[0m, in \u001b[0;36mApiClient.make_request\u001b[0;34m(self, timeout_seconds, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/sae/lib/python3.10/site-packages/httpx/_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.sambanova.ai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     puzzle_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(puzzle_answer)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m modes:\n\u001b[0;32m---> 12\u001b[0m         puzzle_results[mode]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mawait\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mget_puzzle_explanation(mode\u001b[38;5;241m=\u001b[39mmode, explainer_model\u001b[38;5;241m=\u001b[39mEXPLAINER_MODEL_NAME, \n\u001b[1;32m     13\u001b[0m                                                                        puzzle_activation_record\u001b[38;5;241m=\u001b[39mpuzzle\u001b[38;5;241m.\u001b[39mactivation_records))\n\u001b[1;32m     15\u001b[0m puzzle_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(puzzle_results)\n\u001b[1;32m     16\u001b[0m puzzle_df\n",
      "File \u001b[0;32m/workspace/Efficient-LLM-automated-interpretability/neuron-explainer/utils.py:122\u001b[0m, in \u001b[0;36mget_puzzle_explanation\u001b[0;34m(mode, explainer_model, puzzle_activation_record)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    117\u001b[0m     explainer \u001b[38;5;241m=\u001b[39m TokenActivationPairExplainer(\n\u001b[1;32m    118\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mexplainer_model,\n\u001b[1;32m    119\u001b[0m         prompt_format\u001b[38;5;241m=\u001b[39mPromptFormat\u001b[38;5;241m.\u001b[39mHARMONY_V4,\n\u001b[1;32m    120\u001b[0m         max_concurrent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    121\u001b[0m     )\n\u001b[0;32m--> 122\u001b[0m     explanations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m explainer\u001b[38;5;241m.\u001b[39mgenerate_explanations(\n\u001b[1;32m    123\u001b[0m         all_activation_records\u001b[38;5;241m=\u001b[39mpuzzle_activation_record,\n\u001b[1;32m    124\u001b[0m         max_activation\u001b[38;5;241m=\u001b[39mcalculate_max_activation(puzzle_activation_record),\n\u001b[1;32m    125\u001b[0m         num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    129\u001b[0m     explainer \u001b[38;5;241m=\u001b[39m SummaryExplainer(\n\u001b[1;32m    130\u001b[0m       model_name\u001b[38;5;241m=\u001b[39mexplainer_model,\n\u001b[1;32m    131\u001b[0m       prompt_format\u001b[38;5;241m=\u001b[39mPromptFormat\u001b[38;5;241m.\u001b[39mHARMONY_V4,\n\u001b[1;32m    132\u001b[0m       max_concurrent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    133\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/Efficient-LLM-automated-interpretability/neuron-explainer/neuron_explainer/explanations/explainer.py:143\u001b[0m, in \u001b[0;36mNeuronExplainer.generate_explanations\u001b[0;34m(self, num_samples, max_tokens, temperature, top_p, get_token_only, **prompt_kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    141\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt\n\u001b[0;32m--> 143\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mmake_request(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[1;32m    144\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse in generate_explanations is \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;129;01mnot\u001b[39;00m get_token_only):\n",
      "File \u001b[0;32m/workspace/Efficient-LLM-automated-interpretability/neuron-explainer/neuron_explainer/api_client.py:75\u001b[0m, in \u001b[0;36mexponential_backoff.<locals>.decorate.<locals>.f_retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     74\u001b[0m jittered_delay \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39muniform(delay_s \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m jitter), delay_s \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m jitter))\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(jittered_delay)\n\u001b[1;32m     76\u001b[0m delay_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(delay_s \u001b[38;5;241m*\u001b[39m backoff_multiplier, max_delay_s)\n",
      "File \u001b[0;32m~/miniconda3/envs/sae/lib/python3.10/asyncio/tasks.py:605\u001b[0m, in \u001b[0;36msleep\u001b[0;34m(delay, result)\u001b[0m\n\u001b[1;32m    601\u001b[0m h \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mcall_later(delay,\n\u001b[1;32m    602\u001b[0m                     futures\u001b[38;5;241m.\u001b[39m_set_result_unless_cancelled,\n\u001b[1;32m    603\u001b[0m                     future, result)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    607\u001b[0m     h\u001b[38;5;241m.\u001b[39mcancel()\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modes = [\"Original\", \"Summary\", \"Highlight\", \"HighlightSummary\", \"AVHS\"]\n",
    "puzzle_results = {\"puzzle_name\":[], \"answer\":[]}\n",
    "\n",
    "for mode in modes:\n",
    "    puzzle_results[mode] = []\n",
    "\n",
    "for puzzle_name, puzzle in PUZZLES_BY_NAME.items():\n",
    "    puzzle_results[\"puzzle_name\"].append(puzzle_name)\n",
    "    puzzle_answer = puzzle.explanation\n",
    "    puzzle_results[\"answer\"].append(puzzle_answer)\n",
    "    for mode in modes:\n",
    "        puzzle_results[mode].append(await utils.get_puzzle_explanation(mode=mode, explainer_model=EXPLAINER_MODEL_NAME, \n",
    "                                                                       puzzle_activation_record=puzzle.activation_records))\n",
    "   \n",
    "puzzle_df = pd.DataFrame(puzzle_results)\n",
    "puzzle_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzle_df.to_csv(\"test_results/puzzle_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "sae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
